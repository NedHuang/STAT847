---
title: "Experimental Results: Random digits and hypothesis testing"
output:
  pdf_document:
    keep_tex: no
    latex_engine: xelatex
    number_sections: yes
  html_document:
    toc: yes
  word_document: default
geometry: margin=1in
graphics: yes
header-includes:
- \usepackage{graphicx}
- \usepackage{color}
- \usepackage{hyperref}
- \newcommand{\ve}[1]{\mathbf{#1}}    
- \newcommand{\sv}[1]{\boldsymbol{#1}} 
- \newcommand{\mat}[1]{\mathbf{#1}}    
- \newcommand{\sm}[1]{\boldsymbol{#1}}   
- \newcommand{\tr}[1]{{#1}^{\mkern-1.5mu\mathsf{T}}}   
- \newcommand{\wig}[1]{\tilde{#1}}
- \newcommand{\bigwig}[1]{\widetilde{#1}}
- \newcommand{\norm}[1]{||{#1}||}           
- \newcommand{\given}{~\vline~}
- \newcommand{\indep}{\bot\hspace{-.6em}\bot}
- \newcommand{\notindep}{\bot\hspace{-.6em}\bot\hspace{-0.75em}/\hspace{.4em}}
- \newcommand{\depend}{\Join}
- \newcommand{\notdepend}{\Join\hspace{-0.9 em}/\hspace{.4em}}
- \newcommand{\imply}{\Longrightarrow}
- \newcommand{\notimply}{\Longrightarrow \hspace{-1.5em}/ \hspace{0.8em}}
- \newcommand{\code}[1]{\texttt{#1}}
- \newcommand*{\Rnsp}{\textsf{R}}
- \newcommand*{\R}{\textsf{R}$~$}
  
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(12314159)

imageDirectory <- "./img"
dataDirectory <- "./data"
path_concat <- function(path1, path2, sep="/") {
                       paste(path1, path2, sep = sep)
                      }
```

---

**48 marks**

In this question, we will explore the data collected in class.  

You will need to download the data `class_data.csv` from the assignment website and save it somewhere.
Supposing the data to have been saved in a directory `dataDirectory`, read it into \R as

```{r read class data}
# Usual helper for paths
path_concat <- function(path1, path2, sep="/") {
                       paste(path1, path2, sep = sep)
}

data <- read.csv(file = path_concat(dataDirectory, "class_data.csv"))
```

Having loaded the data, you might have a look at its contents using any of the standard functions:

```{r,eval=FALSE}
# just print it
data
# view its contents as a spreadsheet (in RStudio)
View(data)
# or, look at its data structure
str(data)
```

You will find that it is a `data.frame` with `r ncol(class)` variables 

```{r names of variables}
names(data)
```

Each row contains one student's answer to the questions associated with these names.

---

\begin{center}
Here we will only consider the results on the digits (0-9) obtained in class.
\end{center}

---

Recall how the data on the digits 0, 1, \ldots, 9 were collected.

Each person was first asked to write the words "random digit" on a card. They were then given a few seconds to think up a single *random* digit from 0 to 9 and then record it on the card.

On the other side of the card, each person then wrote "student digit" and below it recorded the *last*  digit of their student id number.

These two digits provide the values for the variables `random_digit` and `student_digit` appearing in the data set `data` above.

---

\newpage

1.  Suppose a digit, $d$, is generated as a realization from a random variable $D$ which is uniformly distributed on the digits $\{0, 1, 2, 3, \ldots, 8, 9\}$.  That is, for any value $d \in \{0, 1, 2, 3, \ldots, 8, 9\}$ we have 

\[Pr(D = d) = \frac{1}{10}. \]

    a. *(1 mark)* Determine the expectation $E(D)$.
    
\[
    E(D) = \sum_{D=0}^{9} D\cdot P(D) = \sum_{D=0}^{9} \frac{D}{10} = 4.5
\]
    
    b. *(2 marks)* Determine the expectation $E(D^2)$ and hence the standard deviation $SD(D)$.
\[
  E(D) = \sum_{D=0}^{9} D^2\cdot P(D) = \sum_{D=0}^{9} \frac{D^2}{10} = 28.5
\]

\[
  Var(D) = E(D^2) - E(D)^2 = 28.5 - 4.5^2 = 8.25
\]

\[
  SD(D) = \sqrt(Var(D)) = \sqrt{8.25} = 2.872281
\]

c. *(1 mark)* Determine the median of $D$. 

\[median(D) = 4.5\]
    
    
    d. Suppose we consider the random variable $C$ which for some fixed value $d \in \{0, 1, 2, 3, \ldots, 8, 9\}$ takes values
    
\[C = \left\{ 
            \begin{array}{lcl}
            1 & $~~~~$ & \mbox{when } D = d \\
              &        &                    \\
            0 & $~~~~$ & \mbox{when } D \ne d
            \end{array}
      \right. \]
              
        which implies 
        
\[
Pr(C = 1) = Pr(D = d) = \frac{1}{10}
\]
and  
\[
Pr(C = 0) = Pr(D \ne d) = 1 -  Pr(D = d) = \frac{9}{10}
\]
    
    
        Suppose we have $C_1, C_2, \ldots, C_n$ independent and identically distributed random variables with the same distribution as $C$.
        
        Let $X = \sum_{i=1}^n C_i$.
        
        i. *(1 mark)* What is the name of the probability distribution of $X$?
            - binomial distribution
        
        ii. *(1 mark)* Hence, write down an expression $Pr(X = x)$ for $x \in {0, 1, \ldots, n}$.
        
        
\[
  Pr(X=x) = \binom{n}{k}\left(\frac{1}{10}\right)^{k}\left(1-\frac{1}{10}\right)^{n-k}
\]  
        
        
        
        
        iii. *(1 mark)* Hence, write down an expression $E(X)$.
\[
    E\left(X\right) = np = \frac{n}{10}
\]
        
        
        
        
    e. Using the `data` from class, for **each** of the variables `random_digit` and `student_digit`, calculate (show your code in each case) its
    
        i. *(2 marks)* sample average,
        

```{r}
s_digit <- data$student_digit
r_digit <- data$random_digit
s_digit_mean <- mean(s_digit)
r_digit_mean <- mean(r_digit)
print(s_digit_mean)
print(r_digit_mean)
                    
```
            - The mean of student_digit is 4.595238
            - The mean of random_digit is 5.5
      
      
  
        ii. *(2 marks)* sample standard deviation, 
```{r}
s_digit_sd <- sd(s_digit)
r_digit_sd <- sd(r_digit)
print(s_digit_sd)
print(r_digit_sd)
```
      
      
            - The standard deviation of student_digit is 3.052864
            - The standard deviation of random_digit is 2.778401
        
        
        iii. *(2 marks)* sample median,
        
```{r}
s_digit_median <- median(s_digit)
r_digit_median <- median(r_digit)
print(s_digit_median)
print(r_digit_median)
```
  
          
            - The median of student_digit is 5
            - The median of random_digit is 7
        
        
        
        iv. *(3 marks)* and compare these to the corresponding theoretical values from the distribution of $D$.
            
              Varname                     Theoretical        student_digit        random_digit
              mean                        4.5                4.595238             5.5
              median                      4.5                5                    7
              standard deviation          2.872281           3.052864             2.778401
              
              
              - student_digit sample has larger mean, median, and standard deviation than theoretical value
              - random_digit sample has larger mean, and median than theoretical results. 
              - But its standard deviation is smaller than theoretical value


            
        v. *(1 mark)* Which of `random_digit` and `student_digit` have sample values closer to the theoretical values.
        
            - student_digit have sample values closer to the theoretical values because the median and mean of student_digit
              is closes to theroretical values. student_digit and random_digit have standards deviation are all close to the
              theoretical sd.


        -- Student_digit is more close to theoretical(uniform distribution)

    f. *(3 marks)* Calculate $Pr(X = x)$ when $n =$ `r nrow(data)` and $x = 0$, $5$, and $10$.
\[Pr(X=0) = \binom{42}{0}\left(\frac{1}{10}\right)^{0}\left(\frac{9}{10}\right)^{42-0} = 0.01197251518\]
\[Pr(X=5) = \binom{42}{5}\left(\frac{1}{10}\right)^{5}\left(\frac{9}{10}\right)^{42-5} = 0.17247769726\]
\[Pr(X=10) = \binom{42}{10}\left(\frac{1}{10}\right)^{10}\left(\frac{9}{10}\right)^{42-10} = 0.00505246993\]

---

\newpage

2. We are interested testing the hypothesis
    
    H: the observed digits $d_1$, $d_2$, \ldots, $d_n$ are independent realizations of a random variable $D$ uniformly distributed on the digits $\{0, 1, 2, \ldots, 9 \}$. 

    In particular, we are interested in testing this hypothesis for each of two samples `student_digit` and `random_digit`. 
    
    a. *(4 marks)* The function `stem()` is a simple way to get a quick picture (a "stem and leaf plot") of the distribution of a set of digits.  Use `stem()` to construct a picture of each of the following:
    
        i. the values of `student_digit`,
```{r}
stem(s_digit)
```



        ii. the values of `random_digit`,
```{r}
stem(r_digit)
```
        iii. and, for comparison, a sample of the same size from a uniform distribution on the digits using
            the function `sample()`.

```{r}
uniform_sample <- sample(1:10, replace = TRUE,40)
stem(uniform_sample)
```
        
        
            
        Which of `student_digit` or `random_digit` looks more like it might have come from a Uniform on the digits?  Why?
        
            - student_digit looks more like it might have come from a Uniform on the digits becaus the lengths of 'stems' are similiar. 
            
    b. A more formal way to assess whether a sample of values appear to come from a hypothesized distribution is to calculate the *Pearson's chi-squared test* of "goodness of fit" statistic.  This is generally expressed as
    
\[ \chi^2 = \sum_{i=1}^m \frac{(o_i - e_i)^2}{e_i}  \]
        
        where $o_i$ is the observed number of values in the $i$th "cell", $e_i$ is the expected number to fall into that cell according to the hypothesized model, and $m$ is the total number of (non-overlapping) cells.
        
        In our case, the cells are the 10 different possible digits (so $m = 10$) and $o_i$ is the number of digits $d_1$, $d_2$, \ldots, $d_n$ equal to $i$, for each $i \in \{0, 1, \ldots, 9\}$.  The values $e_i$ are the *expected* number of digits equal to $i$ for each $i \in \{0, 1, \ldots, 9\}$, when the hypothesized model is true.  In this case $e_i = n/m$ for all $i$.
        
        $\Chi^2$ is a discrepancy measure which is larger whenever the $o_i$ are relatively far from their expectation under the model, $e_i$.  The larger is $\Chi^2$, the greater is the evidence against the model.
        
        If the hypothesized model is true, then the distribution of  $\Chi^2$ can usually be approximated as a $\chi^2_k$ distribution having degrees of freedom $k = m - \#\mbox{constraints on the model}$.  In our case, there is only one constraint (the total of the expected values must sum to $n$; $\sum_{i=1}^m e_i =n$). So here, the degrees of freedom are $k = m - 1$.   The usual rule-of-thumb is that $\chi^2_k$ is a good approximation of the distribution of $\Chi^2$ provided $e_i \ge 5$ for all $i = 1, \ldots, m$.
        
        i. *(4 marks)* Write a function `count_digits()` which takes a vector of digits `d` and returns a numeric vector whose $i$th element contains the number of values in `d` which were equal to $i-1$.  That is, write
        
```{r count_digits template, }
count_digits <- function (d) {
  len<- length(d)
  digits <- seq(0,len-1,1)
  return(sapply(digits,function(x) sum(d==x))[1:10])
}



my_digits <- c(0, 1, 3, 4, 7, 1, 4, 9, 7, 4)
print("my_function returns:")
count_digits(my_digits)
# would return a vector equal to
print("it should be: ")
c(1, 2, 0, 1, 3, 0, 0, 2, 0, 1)

```



            # for example if
            my_digits <- c(0, 1, 3, 4, 7, 1, 4, 9, 7, 4)
            # then 
            count_digits(my_digits)
            # would return a vector equal to
            c(1, 2, 0, 1, 3, 0, 0, 2, 0, 1)
            ```
            
            Note, that we will assume that `d` will be an integer vector containing
            only values in $\{0, 1, 2, \ldots , 9 \}$.  
            
            No error checking is required for now.
        
        ii. *(2 marks)* Demonstrate your function on the digits of the variable `student_digit` and  on the digits of the variable `student_digit`.
```{r}
count_digits(s_digit)
count_digits(r_digit)

```
        
        iii. *(4 marks)* Write the function `Pearson_chi_sq(observed, expected)` which calculates $\Chi^2$ where `observed` is a vector of observed counts, and `expected` is a vector of expected counts given the model.  The vector `expected` should have the same length as `observed` or be of length 1.
        
                Again, for expediency it will be assumed that both `observed` and `expected` vectors contain only integer elements in $\{0, 1, 2, \ldots, 9\}$.
            
                However, you should check that the lengths of `observed` and `expected` match and stop if they do not.  If the length of `expected` is only 1, then create a vector of length equal to that of `observed` with the value of `expected` repeated.
            
            
```{r chi_squared templat}
Pearson_chi_sq <- function (observed, 
                        expected = sum(observed)/length(observed)) {
  if(length(expected) == 1){
    expected = rep(expected,length((observed)))
  }
  if(length(observed) != length(expected)){
    print("Lengths do not match")
    return()
  }
  values <- mapply(function(x,y){(x-y)^2/y}, observed, expected)
  return(sum(values))
}

Pearson_chi_sq(count_digits(data$student_digit))
Pearson_chi_sq(count_digits(data$random_digit))
```
     
     
     
            - The Pearson_chi_sq
            - 
        iv. *(2 marks)* Check your function by comparing the values of `Pearson_chi_sq(observed)`  to results of `chisq.test(observed)$statistic` for `observed` being each of 
            
            `count_digits(data$student_digit)` and `count_digits(data$random_digit)` in turn.
```{r}
chisq.test(count_digits(data$student_digit))$statistic

chisq.test(count_digits(data$random_digit))$statistic

```

        v. *(2 marks)* Using the function `pchisq()` calculate the $p$-value testing the uniformity hypothesis using your calculated `Pearson_chi_sq()` value for the counts of the digits in each of the `data` variables `student_digit` and `random_digit`.  
        Show that your 𝑝-values agree with those produced by chisq.test(observed)$p.value for the counts of the digits in each of the data variables student_digit and random_digit as observed.
        
```{r}
pchisq(Pearson_chi_sq(count_digits(data$student_digit)),9,lower.tail = FALSE)
pchisq(Pearson_chi_sq(count_digits(data$random_digit)),9,lower.tail = FALSE)

chisq.test(count_digits(data$student_digit))$p.value
chisq.test(count_digits(data$random_digit))$p.value
```



        vi. *(3 marks)* Rather than depend upon the validity of the $\chi^2_k$ approximation, we could *simulate* the distribution of $\Chi^2$ by calculating its value on $B$ samples of size `n = nrow(data) =` `r nrow(data)` generated by the function `sample()`.
        
            Using the function `sapply()` and the function `sample()`, together with your functions `Pearson_chi_sq()` and `count_digits()` to create a function `get_chisqs(n, B = 1000)` that generates the chi-squared statistic on each of `B` independently drawn samples, each being of size `n` from a uniform distributon on the digits $\{0, 1, \ldots , 9 \}$, and returns a numeric vector of length `B` containing the `B` chi-squared statistics.
            
            That is, write
            
            
```{r}

get_chisqs <- function (n, B = 1000) {
  count_list <- list()
  for (i in 1:B){ 
    my_sample <- sample(0:9, n, replace = TRUE)
    count_list[[i]] <- count_digits(my_sample)
  }
  chisq_list<-sapply(1:B, function(x)
  Pearson_chi_sq(count_list[[x]]))
  return(chisq_list)
}

n <- nrow(data)
results <- get_chisqs(n = n, B = 1000)

#print(results)
```



         
        vii. *(3 marks)* Use your function `get_chisqs()` to get  `B = 10000` independent pseudo-random realizations of the $\Chi^2$ statistic from a sample of size `n = nrow(data) =` `r nrow(data)` from the uniform distribution on the digits.  
        
                That is, execute the following (N.B. in RMarkdown change header to eval = TRUE):
        
```{r}
n <- nrow(data)
B <- 10000 # TEN thousand
set.seed(314159) # So we all get the same values
chisq_stats <- get_chisqs(n = n, B = B)
hist(chisq_stats, 
     col = "lightgrey",
     main = "Simulated Pearson test null distribution",
     xlab = "test stat")
student_line <- Pearson_chi_sq(count_digits(data$student_digit))
#print(student_line)
random_line <- Pearson_chi_sq(count_digits(data$random_digit))
#print(random_line)
abline(v=student_line,col='red',lty=2)
abline(v=random_line,col='blue',lty=2)
legend("topright",legend=c('student_digit','random_digit'),col=c('red','blue'),lty =2)
```



        
                To this histogram, add a vertical line in "red" where the corresponding statistics you calculated should be for the `student_digit` variable, and add a vertical line in "blue"  where the corresponding statistics you calculated should be for the `random_digit` variable.
        
                Put a legend in the top right that identifies the lines.
        
                Based on this histogram, which collection of digits seem less likely to have been generated as a random sample from a uniform distribution of digits?  Why?
                    - Random digit is lesslikely to have been generated as a random sample.
                    - The larger is X2, the greater is the evidence against the model.
                    - random_digit has large X2 value.
          
        viii. *(2 marks)* The simulated distributions can be used to calculate approximate $p$-values by simply evaluating the proportion of the simulated test statistics which are greater than or equal to the $\Chi^2$ values for each of the counts corresponding to  `student_digit` and `random_digit`.
        
                Calculate these two $p$-values using the simulated test null distribution given by the vector `chisq_stats` above.  Show your code.
```{r}
chi_sq_student <- Pearson_chi_sq(count_digits(data$student_digit))
p_value_student <- sum(chisq_stats >= chi_sq_student)/length(chisq_stats)
print(p_value_student)

chi_sq_random <- Pearson_chi_sq(count_digits(data$random_digit))
p_value_random <- sum(chisq_stats >= chi_sq_random)/length(chisq_stats)
print(p_value_random)
```
          
        ix. *(2 marks)*   What do you conclude about the two hypotheses?
        
          - For student digit, the p-value > 0.05, which means that the nulll hypothesis should be rejected. 
          - For random digit, the p-value < 0.05, which means that the null hypothesis can be accepted.
          - student_digit is NOT randomly selected, the random_digit is randomly selected.